{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics Vidhya Janatahack: Customer Segmentation\n",
    "\n",
    "This notebook is in reference to the Janatahack Competition on customer segmentation conducted by Analytics Vidhya on  31-07-2020.\n",
    "\n",
    "The given model performed a testing accuracy of 75% and has acquired rank 145 on LB.\n",
    "\n",
    "Customer segmentation is the practice of dividing a customer base into groups of individuals that are similar in specific ways relevant to marketing, such as age, gender, interests and spending habits. Companies employing customer segmentation operate under the fact that every customer is different and that their marketing efforts would be better served if they target specific, smaller groups with messages that those consumers would find relevant and lead them to buy something. Companies also hope to gain a deeper understanding of their customers' preferences and needs with the idea of discovering what each segment finds most valuable to more accurately tailor marketing materials toward that segment.\n",
    "\n",
    "More info: https://datahack.analyticsvidhya.com/contest/janatahack-customer-segmentation/#About\n",
    "\n",
    "NB: Not claiming this is one of the best models. I took the competition as a learning experience. Additional suggestions and insights are invited on where can I improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "An automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4 and P5). After intensive market research, theyâ€™ve deduced that the behavior of new market is similar to their existing market.\n",
    "\n",
    "In their existing market, the sales team has classified all customers into 4 segments (A, B, C, D ). Then, they performed segmented outreach and communication for different segment of customers. This strategy has work exceptionally well for them. They plan to use the same strategy on new markets and have identified 2627 new potential customers. \n",
    "You are required to help the manager to predict the right group of the new customers.\n",
    "\n",
    "More info : https://datahack.analyticsvidhya.com/contest/janatahack-customer-segmentation/#ProblemStatement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/analytics-vidhya-janatahack-customer-segmentation/Train_aBjfeNk.csv')\n",
    "test = pd.read_csv('../input/analytics-vidhya-janatahack-customer-segmentation/Test_LqhgPWU.csv')\n",
    "train_copy = train.copy()\n",
    "test_copy = test.copy()\n",
    "train_copy = train_copy.drop(['Segmentation'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both training and test data should be undergone preprocessing both are concatenated into a single data frame 'data'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinating train and test data for prperpcessing purposes\n",
    "train_copy['train'] = 1\n",
    "test_copy['train'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatinating the train and test data\n",
    "data = pd.concat([train_copy,test_copy], axis = 0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there are any missing values in training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, for the sake of simplicity, the missing values are filled with **mode value of the training data**. Care should be taken not to take the mode values of the whole 'data' dataframe to avoid information leakage from test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating missing values\n",
    "# For the sake of time just fill the missing values using mean or mode\n",
    "data['Ever_Married'] = data['Ever_Married'].fillna(train_copy['Ever_Married'].mode()[0])\n",
    "data['Graduated'] = data['Graduated'].fillna(train_copy['Graduated'].mode()[0])\n",
    "data['Profession'] = data['Profession'].fillna(train_copy['Profession'].mode()[0])\n",
    "data['Work_Experience'] = data['Work_Experience'].fillna(train_copy['Work_Experience'].mode()[0])\n",
    "data['Family_Size'] = data['Family_Size'].fillna(train_copy['Family_Size'].mode()[0])\n",
    "data['Var_1'] = data['Var_1'].fillna(train_copy['Var_1'].mode()[0])\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the following variables can be  label encoded\n",
    "* Gender\n",
    "* Ever_Married\n",
    "* Graduated\n",
    "* Spending_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding the variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "columns = ['Gender','Ever_Married','Graduated']\n",
    "for col in columns:\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable 'Spending_Score' has three values : Low, Average and High. Since they are ordinal variables, they have to be label encoded separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Spending_Score'] = data['Spending_Score'].map({'Low': 0, 'Average':1, 'High':2}) \n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the data, there are two additional categorical variables: 'Profession' and 'Var_1'. Since they are nominal variables, one hot encoding is to be performed. But, before encoding them directly, lets take a look at the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " train_copy.Profession.value_counts()\n",
    "# temp\n",
    "\n",
    "# data['Profession_counts'] = data['Profession'].apply(lambda x: temp[x])\n",
    "# data[['Profession','Profession_counts']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, len(data['Profession'])):\n",
    "#     if (data.iloc[i]['Profession_counts'] < 1000):\n",
    "#         data['Profession'][i] = 'Others'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable 'Profession' has 9 categories. But among these 9 categories, only initial 3-4 are the most frequent. Hence, rest if them are binned into a new category named 'Other'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Profession'] = data['Profession'].replace(['Lawyer','Executive','Marketing','Homemaker'],'Other')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same strategy is applied for the variable 'Var_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy['Var_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Var_1'] = data['Var_1'].replace(['Cat_5','Cat_1','Cat_7','Cat_2'],'Other')\n",
    "data['Var_1'].value_counts()\n",
    "# data.drop(['Profession_counts'], axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, applying the one hot encoding,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns = ['Profession','Var_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############### Temporary, remove after check ########################\n",
    "# 'Var_1_Cat_3' 'Profession_Doctor\n",
    " \n",
    "# data = data.drop(['Profession_Engineer','Var_1_Other',], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Age_Work_product'] = data['Age'] * data['Work_Experience']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing, the training and test data are separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_preprocessed  = data[data['train']==1]\n",
    "training_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_preprocessed['Segmentation'] = train.Segmentation\n",
    "#training_preprocessed['Segmentation'] = training_preprocessed['Segmentation'].map({'A':0,'B':1,'C':2,'D':3})\n",
    "training_preprocessed = training_preprocessed.drop(['train'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting dependent and independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into dependent and independent variables\n",
    "X_train = training_preprocessed.drop(['Segmentation'],axis = 1)\n",
    "Y_train = training_preprocessed['Segmentation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data[data['train'] == 0]\n",
    "X_test = X_test.drop(['train'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def get_score(model,x_train, x_test, y_train, y_test):\n",
    "    model.fit(x_train,y_train)\n",
    "    y_predict = model.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "lgbm = LGBMClassifier()\n",
    "catb = CatBoostClassifier()\n",
    "\n",
    "# model = lr\n",
    "# learning_rates = [0.001,0.01, 0.1]# 0.01, 0.1\n",
    "# n_estimators   = [10, 100, 1000]\n",
    "# # subsample = [0.5, 0.7, 1.0]\n",
    "# # max_depth = [3, 7, 9]\n",
    "# grid = dict(learning_rate = learning_rates, n_estimators = n_estimators)\n",
    "# cv = RepeatedStratifiedKFold(n_splits = 6)\n",
    "# grid_search = GridSearchCV(model, param_grid = grid,  cv=cv, scoring='accuracy')\n",
    "# opt_param_result = grid_search.fit(X_train,Y_train)\n",
    "\n",
    "# print('Best :{} using parameters:{}'.format(grid_search.best_score_,grid_search.best_params_))\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits = 6)\n",
    "score_xgb= []\n",
    "# score_lgbm = []\n",
    "# score_catb = []\n",
    "# score_ada = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    x_train, x_val, y_train, y_val = X_train.iloc[train_index],X_train.iloc[test_index],Y_train.iloc[train_index],Y_train.iloc[test_index]\n",
    "    score_xgb.append(get_score(xgb,x_train, x_val, y_train, y_val))\n",
    "#     #score_lgbm.append(get_score(lgbm,x_train, x_val, y_train, y_val)) #Using LGBMBooster\n",
    "#     #score_xgb.append(get_score(xgb,x_train, x_val, y_train, y_val)) #Using XGBooster\n",
    "\n",
    "print('Accurcay for Bagging is {} ({})'.format(np.mean(score_xgb), np.std(score_xgb)))\n",
    "# # print('Accurcay for xgb is {} ({})'.format(np.mean(score_xgb), np.std(score_xgb)))\n",
    "# # print('Accurcay for catb is {} ({})'.format(np.mean(score_catb), np.std(score_catb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional evaluations are done on LGBM and CatBoost Classifiers . Yet, I was't able to get the training accurcay more than 53%. Even with additional hyper parameter tuning on XGB classifier, the training accuracy was not improved. Hence, decided to stick with the XGB classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training using the whole dataset\n",
    "xgb.fit(X_train,Y_train)\n",
    "predict = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance \n",
    "plot_importance(xgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the feature importance, I tried to remove some  the less important variables and checked the performance. But the accuracy got decreased. Hence those features are  kept intact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the test results to a separate dataframe\n",
    "submission = pd.DataFrame()\n",
    "submission['ID'] = test_copy['ID']\n",
    "submission['Segmentation'] = predict\n",
    "submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, I removed the 'ID' variable and perfomed the training resulting in very low accuracy. Also, I tried to do some more feature engineering  and hyper parameter tuning to improve the training accuracy. But even then, the accuracy couldn't be improved. So, I submitted the baseline model itself as the final result and got 74% testing accuracy. Hope someone could provide additional insight into that.\n",
    "\n",
    "Since I am a newbie to this field, the hackathon was a nice learning experience.  \n",
    "\n",
    "Additional suggestions and insights are invited on where can I improve.\n",
    "\n",
    "Thank you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
